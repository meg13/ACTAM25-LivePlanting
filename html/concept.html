<!DOCTYPE html>
<html lang="en"> 
    <head>
        <title>Concept</title>
        <meta charset="UTF-8">
        <link rel="preload" href="../images/branch.svg" as="image">
        <link href="../css/concept_style.css" type="text/css" rel="stylesheet"/>
        <link href="../css/style.css" type="text/css" rel="stylesheet"/>
    </head>
    <body>
        <header>
        <h1>CONCEPT</h1>
        </header>
        <main>
            <div class="content-wrapper">

                <article>
                    <h2>Introduction</h2>
                        <p>What if we could translate into sounds the behavior of plants? What if we could make them talk, or even sing? That is the core idea behind any kind of Biodata Sonification project. 
                            Biodata Sonification does not revolve around any true "voice" of a plant, but rather around mapping in terms of audible sounds or even in musical terms the signals that can be measured from a plant. 
                            We decided to center our project for the Advanced Coding Tools and Methodologies Course around the creation of our own version of a Biodata Sonification project, through the implementation of <b>Python</b> 
                            for audio coding and <b>HTML-CSS-JavaScript</b> for webpage coding. 
                         </p>

                        <p>We chose to map as sounds only the humidity level of the soil and the bioelectrical impulses in the leaves. To detect these signals we used a <b>APKLVSR sensor</b> for humidity and <b>TENS electrode</b> combined with 
                            a <b>Symbiotic Kit</b> for the electric impulses. The Symbiotic Kit, produced by Spad Electronics, is a microcontroller that measures the tiny electrical fluctuations from plant leaves through the electrodes, 
                            and then converts those signals into numerical MIDI/CV/gate/trigger data. We did not use the The Symbiotic Kit as combined with a true synthesizer: instead we linked both 
                            the APKLVSR sensor and the Symbiotic Kit to an <b>Arduino MEGA</b> microcontroller board, which reads and standardizes the numerical outputs of both sensors and sends those numbers (as sequences of bits) to the computer. 
                            Inside the computer, a code in Python language is responsible, instead of a synthesizer, of the actual sound generation. 
                        </p>
                </article>

                <article>
                    <h2>Python</h2>
                        <p>The python code, through another code running on an <b>Arduino IDE</b>, reads the numbers corresponding to the humidity and electrical voltage that Arduino MEGA constantly sends. The code is also responsible for 
                            defining a digital synthesizer—a software-based instrument that generates sound waves—which in turn plays the mapped signals. The mapping of the different signals is meant to mimic the way in nature humidity defines the overall, long term condition of the plant, 
                            while bioelectrical impulses represent more instantaneous and quick occurrences. But how are the signals actually mapped? 
                         </p>

                        <p>For bioelectrical impulses, we associated the range <code>0-1023</code> (in terms of Arduino values) to a range of five octaves. This musical range is expressed in terms of semitones from a pre-set base root frequency. 
                            Every time an impulse number is received after a certain cooldown time, it is converted to the respective number of semitones above the base root, which is then rounded to the closest number of semitones in a major scale. 
                            This number is then used to define a note which is played by the synth. In other words, the higher the voltage detected, the higher the frequency of the respective note. The notes associated with the bioelectrical 
                            impulses have a shorter duration and limited attack and release. The code also offers the possibility to interact with the pulse notes, by allowing the user to record and loop up to five segments of bioelectrical-impulse 
                            audio, and resetting those loops when desired. If you are an user, you are encouraged to touch the plant's leaves and see how the pulse notes change, as long as you are delicate! 
                        </p>

                        <p>The humidity values are mapped in a similar way but used differently from the bioelectrical impulses. Since we observed that the humidity values typically recorded almost always sit in the range <code>200-400</code>, we associated 
                            to it another range of three octaves above another base root, in major scale. However, in contrast with the electric impulses, the code iterates by recording every received humidity value for the length of a timer. 
                            Once that temporal length has been surpassed, all of these values are averaged and an "ambience note" is built accordingly from the average. A higher local humidity corresponds to a higher frequency. 
                            Ambience notes have slower attacks than pulse notes, a more pronounced vibrato and they are played continuously until user interruption. Therefore the user, by placing the very sensitive APKLVSR sensor in different 
                            points of the plant's soil over time, can build various layers of an ambience that lies underneath the pulse notes/loops, and can be reset when necessary. 
                         </p>
                </article>
                   
                <article>
                    <h2>Web Interface and Visual Design</h2>
                        <p>The user interface of our project was developed as a multi-page web application that combines real-time audio visualization with interactive animations inspired by 8-bit video games aesthetics. 
                            The entire visual design was initially prototyped in Figma and subsequently implemented using HTML, CSS and JavaScript.
                        </p>
                        <p>
                            The <b>Home page</b> serves as the entry point with a clear "START" button in retro gaming style, surrounded by animated pixel-art animals (cows and ducks) that move across the screen. 
                            All animal graphics were custom-designed as SVG vector images in Figma. 
                            Each animal is implemented through dedicated JavaScript classes (<code>SimpleCow</code> and <code>SimpleDuck</code>) that feature random movement patterns, collision detection, and pause behaviors.
                        </p>
                        <p>
                            The <b>Live Listening</b> page represents the core of the user interface, where real-time interaction with the plant's biodata occurs. 
                            This page is centered around a large canvas-based waveform visualization that provides immediate visual feedback of the audio signal being generated starting from the plant's sensors.
                        </p>
                        <p>
                            The waveform rendering is implemented using the <b>HTML5 Canvas API</b> for high-performance graphics. 
                            The visualization operates through a circular buffer system that stores 256 audio samples in a Float32Array. 
                            This buffer continuously updates with new data while discarding the oldest samples, creating a smooth scrolling effect. 
                            The actual rendering occurs through <code>requestAnimationFrame</code> at 60fps, ensuring fluid animation synchronized with the browser's refresh rate.
                            For each frame, the canvas displays the waveform as a continuous blue line that traces the audio signal's amplitude. 
                            Audio samples, arriving as normalized float values from -1.0 to +1.0, are mapped to vertical canvas coordinates, with horizontal spacing calculated to distribute the waveform evenly across the display width. 
                            When no audio is playing, a static horizontal line appears at the center as a visual baseline.

                        </p>
                        <p>
                            The page features a set of interactive controls positioned below the waveform visualization. 
                            The <b><i>Start Listening</i></b> and <b><i>Stop Listening</i></b> buttons manage the audio stream, complemented by recording controls that allow users to capture loops. 
                            The <b><i>Record Loop</i></b> button provides visual feedback by changing color during active recording. Separate controls enable clearing accumulated loops or resetting ambient notes. 
                        </p>
                        <p>
                            The <b>Code</b> page contains comprehensive documentation of both frontend and backend implementation, with syntax-highlighted code snippets rendered using <b>Prism.js</b>. 
                        </p>
                </article>

                <article>
                    <h2>Backend Architecture and Web Communication</h2>
                        <p>The communication between the Python audio engine and the web interface is achieved through a dual-protocol architecture. The system uses two different communication protocols to separate control commands from data streaming, 
                            ensuring both reliable command execution and real-time visualization.
                        </p>
                        <p>
                            The Python backend implements an <b>HTTP server</b> running on localhost port 8080, which handles all user commands sent from the web interface. When a user clicks on buttons like <b><i>Start Listening</i></b>, 
                            <b><i>Stop Listening</i></b>, <b><i>Record Loop</i></b>, or the clearing controls, the JavaScript frontend sends POST requests (HTTP commands that trigger actions) to specific endpoints (<code>/start</code>, <code>/stop</code>, 
                            <code>/start_rec</code>, <code>/stop_rec</code>, <code>/clear_loops</code>, <code>/clear_ambient</code>). This HTTP-based command system guarantees that user interactions are reliably processed 
                            regardless of network conditions, as each command receives an explicit JSON response confirming its execution.
                        </p>
                        <p>
                            Parallel to the HTTP command channel, the Python backend establishes a <b>WebSocket server</b> on port 8765 dedicated exclusively to streaming audio visualization data. Once the audio generation starts, 
                            the Python code continuously sends chunks of audio samples as binary data through the WebSocket connection. These samples, formatted as arrays of floating-point numbers (Float32Array), are received by the JavaScript 
                            <code>LiveAudioController</code> class in real-time and fed into the circular buffer—a rotating memory storage that continuously updates with new data while discarding the oldest—that powers the canvas-based waveform visualization. 
                            This separation of concerns—HTTP for commands, WebSocket for data streaming—allows the web interface to maintain responsive controls even during intensive audio processing.
                        </p>
                        <p>
                            The WebSocket connection is treated as optional in the frontend implementation: if the WebSocket server is unavailable, the audio playback continues through the system's speakers and only the 
                            visualization is affected. This design choice ensures that the core functionality of listening to the plant's biodata remains accessible even in degraded network conditions.
                        </p>
                        <p>
                            The <code>LiveAudioController</code> JavaScript class orchestrates the entire client-side interaction. It manages the canvas rendering loop through <code>requestAnimationFrame</code>, processes 
                            incoming WebSocket messages, maintains the visualization buffer state, and synchronizes the UI elements (button states, recording indicators) with the backend's operational status. The class 
                            implements error handling for both HTTP and WebSocket failures, providing user feedback when connections cannot be established.
                        </p>
                        <p>
                            This architecture effectively bridges the gap between the physical sensors connected to Arduino, the Python-based audio synthesis engine, and the interactive web interface, creating a seamless 
                            user experience where botanical data transforms into musical expression in real-time.
                        </p>
                </article>

                
            </div>
        </main>
    </body>
    <script src="../js/nav.js" type="text/javascript"></script>
</html>
